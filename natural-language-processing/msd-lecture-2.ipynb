{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8515f57b",
   "metadata": {},
   "source": [
    "## Document classification notebook\n",
    "\n",
    "This notebook illustrates the lecture on document classification. We will go through the steps towards basic\n",
    "document classification systems with a bag of words representation and tf-idf weighting associaed with k-nn search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a308f2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gonem/miniforge3/envs/ENSAI/lib/python3.8/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/gonem/miniforge3/envs/ENSAI/lib/python3.8/site-packages/torch/cuda/__init__.py:740: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /opt/conda/conda-bld/pytorch_1695392036766/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No GPU devices detected",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/gonem/ENSAI/NLP/msd-lecture-2.ipynb Cell 2\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gonem/ENSAI/NLP/msd-lecture-2.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gonem/ENSAI/NLP/msd-lecture-2.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gonem/ENSAI/NLP/msd-lecture-2.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m spacy\u001b[39m.\u001b[39;49mrequire_gpu()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gonem/ENSAI/NLP/msd-lecture-2.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gonem/ENSAI/NLP/msd-lecture-2.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m process \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39men_core_web_md\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/ENSAI/lib/python3.8/site-packages/thinc/util.py:232\u001b[0m, in \u001b[0;36mrequire_gpu\u001b[0;34m(gpu_id)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot use GPU, CuPy is not installed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m has_gpu:\n\u001b[0;32m--> 232\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo GPU devices detected\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    234\u001b[0m \u001b[39mif\u001b[39;00m has_cupy_gpu:\n\u001b[1;32m    235\u001b[0m     set_current_ops(CupyOps())\n",
      "\u001b[0;31mValueError\u001b[0m: No GPU devices detected"
     ]
    }
   ],
   "source": [
    "#\n",
    "# load a bunch of modules\n",
    "#\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import spacy\n",
    "spacy.require_gpu()\n",
    "from tqdm import tqdm\n",
    "process = spacy.load('en_core_web_md')\n",
    "print(process.pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a020130",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The data that we will use comes from a super popular toy dataset for opinion mining: IMDb. The dataset provides reviews from the IMDb website where half of the reviews express positive comments and half negative. The task is this geared towards detecting the _polarity_ of the opinion expressed.\n",
    "\n",
    "See https://ai.stanford.edu/~amaas/data/sentiment for details. You can also get the raw data from there but I'm providing you with an easier json version to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc257064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of utterances = 25000\n",
      "number of + utterances = 12500\n",
      "number of - utterances = 12500\n",
      "data[0] = ['pos', 'For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.']\n",
      "data[-1] = ['neg', 'Not that I dislike childrens movies, but this was a tearjerker with few redeeming qualities. M.J. Fox was the perfect voice for Stuart and the rest of the talent was wasted. Hugh Laurie can be amazingly funny, but is not given the chance in this movie. It´s sugar-coated sugar and would hardly appeal to anyone over 7 years of age. See Toy Story, Monsters Inc. or Shrek instead. 3/10']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# load IMDb data\n",
    "#\n",
    "fn = 'imdb-trn.json'\n",
    "\n",
    "with open(fn, 'rt') as f:\n",
    "    imdb_data = json.load(f)\n",
    "    \n",
    "print('number of utterances =', len(imdb_data))\n",
    "print('number of + utterances =', len([x for x in imdb_data if x[0] == 'pos']))\n",
    "print('number of - utterances =', len([x for x in imdb_data if x[0] == 'neg']))\n",
    "print('data[0] =', imdb_data[0])\n",
    "print('data[-1] =', imdb_data[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5515674b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of utterances = 2000\n",
      "number of + utterances = 1000\n",
      "number of - utterances = 1000\n",
      "data[0] = ['pos', 'For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.']\n",
      "data[-1] = ['neg', 'Not that I dislike childrens movies, but this was a tearjerker with few redeeming qualities. M.J. Fox was the perfect voice for Stuart and the rest of the talent was wasted. Hugh Laurie can be amazingly funny, but is not given the chance in this movie. It´s sugar-coated sugar and would hardly appeal to anyone over 7 years of age. See Toy Story, Monsters Inc. or Shrek instead. 3/10']\n",
      "number of test utterances = 400\n",
      "number of + test utterances = 200\n",
      "number of - test utterances = 200\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Make a smaller dataset so we can go fast, splitting into train and test.  \n",
    "#\n",
    "\n",
    "ntrain_per_class = 1000\n",
    "ntest_per_class = 200\n",
    "\n",
    "imdb_data_small = imdb_data[:ntrain_per_class] + imdb_data[-ntrain_per_class:]\n",
    "nutterances = len(imdb_data_small)\n",
    "\n",
    "print('number of utterances =', nutterances)\n",
    "print('number of + utterances =', len([x for x in imdb_data_small if x[0] == 'pos']))\n",
    "print('number of - utterances =', len([x for x in imdb_data_small if x[0] == 'neg']))\n",
    "print('data[0] =', imdb_data_small[0])\n",
    "print('data[-1] =', imdb_data_small[-1])\n",
    "\n",
    "n1 = ntrain_per_class\n",
    "n2 = ntrain_per_class + ntest_per_class\n",
    "imdb_data_test = imdb_data[n1:n2] + imdb_data[-n2:-n1]\n",
    "print('number of test utterances =', len(imdb_data_test))\n",
    "print('number of + test utterances =', len([x for x in imdb_data_test if x[0] == 'pos']))\n",
    "print('number of - test utterances =', len([x for x in imdb_data_test if x[0] == 'neg']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff9f846",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "To make things faster, we will pre-process the data (tokenization, POS tagging, lemmatization) with spaCy's English pipeline. If you have some time to waste, you can try to do that explictly with NLTK's tokenizer, POS tagger and lemmatizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44eeb684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'pos', 'token': ['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', '.', 'imagine', 'a', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny', '!', 'maureen', 'stapleton', 'is', 'a', 'scene', 'stealer', '.', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream', '.', 'watch', 'for', 'alan', '\"', 'the', 'skipper', '\"', 'hale', 'jr', '.', 'as', 'a', 'police', 'sgt', '.'], 'pos': ['IN', 'DT', 'NN', 'WDT', 'VBZ', 'DT', 'NN', 'RB', 'RB', 'VBP', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'VBN', 'IN', 'DT', 'NN', '.', 'VB', 'DT', 'NN', 'WRB', 'NNP', 'NNP', 'VBZ', 'RB', 'JJ', '.', 'NNP', 'NNP', 'VBZ', 'DT', 'NN', 'NN', '.', 'DT', 'NNP', 'NN', 'VBZ', 'DT', 'JJ', 'NN', '.', 'VB', 'IN', 'NNP', '``', 'DT', 'NNP', \"''\", 'NNP', 'NNP', 'NNP', 'IN', 'DT', 'NN', 'NNP', '.'], 'lemma': ['for', 'a', 'movie', 'that', 'get', 'no', 'respect', 'there', 'sure', 'be', 'a', 'lot', 'of', 'memorable', 'quote', 'list', 'for', 'this', 'gem', '.', 'imagine', 'a', 'movie', 'where', 'Joe', 'Piscopo', 'be', 'actually', 'funny', '!', 'Maureen', 'Stapleton', 'be', 'a', 'scene', 'stealer', '.', 'the', 'Moroni', 'character', 'be', 'an', 'absolute', 'scream', '.', 'watch', 'for', 'Alan', '\"', 'the', 'Skipper', '\"', 'Hale', 'jr', '.', 'as', 'a', 'police', 'Sgt', '.']}\n",
      "{'label': 'neg', 'token': ['not', 'that', 'i', 'dislike', 'childrens', 'movies', ',', 'but', 'this', 'was', 'a', 'tearjerker', 'with', 'few', 'redeeming', 'qualities', '.', 'm.j.', 'fox', 'was', 'the', 'perfect', 'voice', 'for', 'stuart', 'and', 'the', 'rest', 'of', 'the', 'talent', 'was', 'wasted', '.', 'hugh', 'laurie', 'can', 'be', 'amazingly', 'funny', ',', 'but', 'is', 'not', 'given', 'the', 'chance', 'in', 'this', 'movie', '.', 'it´s', 'sugar', '-', 'coated', 'sugar', 'and', 'would', 'hardly', 'appeal', 'to', 'anyone', 'over', '7', 'years', 'of', 'age', '.', 'see', 'toy', 'story', ',', 'monsters', 'inc.', 'or', 'shrek', 'instead', '.', '3/10'], 'pos': ['RB', 'IN', 'PRP', 'VBP', 'NNS', 'NNS', ',', 'CC', 'DT', 'VBD', 'DT', 'NN', 'IN', 'JJ', 'VBG', 'NNS', '.', 'NNP', 'NNP', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'CC', 'DT', 'NN', 'IN', 'DT', 'NN', 'VBD', 'VBN', '.', 'NNP', 'NNP', 'MD', 'VB', 'RB', 'JJ', ',', 'CC', 'VBZ', 'RB', 'VBN', 'DT', 'NN', 'IN', 'DT', 'NN', '.', 'NNP', 'NN', 'HYPH', 'VBN', 'NN', 'CC', 'MD', 'RB', 'VB', 'IN', 'NN', 'IN', 'CD', 'NNS', 'IN', 'NN', '.', 'VB', 'NNP', 'NNP', ',', 'NNPS', 'NNP', 'CC', 'NNP', 'RB', '.', 'CD'], 'lemma': ['not', 'that', 'I', 'dislike', 'children', 'movie', ',', 'but', 'this', 'be', 'a', 'tearjerker', 'with', 'few', 'redeem', 'quality', '.', 'M.J.', 'Fox', 'be', 'the', 'perfect', 'voice', 'for', 'Stuart', 'and', 'the', 'rest', 'of', 'the', 'talent', 'be', 'waste', '.', 'Hugh', 'Laurie', 'can', 'be', 'amazingly', 'funny', ',', 'but', 'be', 'not', 'give', 'the', 'chance', 'in', 'this', 'movie', '.', 'It´s', 'sugar', '-', 'coat', 'sugar', 'and', 'would', 'hardly', 'appeal', 'to', 'anyone', 'over', '7', 'year', 'of', 'age', '.', 'see', 'Toy', 'Story', ',', 'Monsters', 'Inc.', 'or', 'Shrek', 'instead', '.', '3/10']}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We will process the text with spaCy for tokenization, POS tagging and lemmatization. We will process each text\n",
    "# in the database and create a new representation of the database as a list where each entry is a dictionary with\n",
    "# label, tokens, POS Tags, lemmas.\n",
    "#\n",
    "# For efficiency reasons, we call spaCy's processing in a special way (with pipe() that enables parallelization)\n",
    "# and disable the elements of the pipeline we will not use (i.e, depency parsing and named entity recognition).\n",
    "# This results in a list of processed texts from which we can read the tokens, POS tags and lemmas as we did\n",
    "# for lecture 1.\n",
    "#\n",
    "\n",
    "database = imdb_data_small\n",
    "nutterances = len(database)\n",
    "\n",
    "raw_texts = [x[1] for x in database]\n",
    "processed_texts = list(process.pipe(raw_texts, disable=['parser', 'ner']))\n",
    "    \n",
    "# initialize output structure\n",
    "data = []\n",
    "\n",
    "\n",
    "for i in range(nutterances):\n",
    "    buf = {}\n",
    "    buf['label'] = database[i][0]\n",
    "    buf['token'] = [t.text.lower() for t in processed_texts[i]]\n",
    "    buf['pos'] = [t.tag_ for t in processed_texts[i]]\n",
    "    buf['lemma'] = [t.lemma_ for t in processed_texts[i]]    # already lowercased\n",
    "    \n",
    "    data.append(buf)\n",
    "\n",
    "print(data[0])\n",
    "print(data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d864ccaa",
   "metadata": {},
   "source": [
    "### Choose vocabulary\n",
    "\n",
    "The _vocabulary_ is the list of terms that we will consider to represent the documents. We'll limit ourselves to simple terms (as opposed to complex terms such as 'can opener' or 'neural network') and simply select the most frequent terms in the corpus.\n",
    "\n",
    "We provide the function get_token_counts() to help  doing that. The function takes the database as input and returns a list of tokens therein (no filtering in initial version) with the number of times they appear. Output\n",
    "is a dictionary with token: count sorted in descending order w.r.t. the number of occurrences in the database \n",
    "\n",
    "Note that many toolkits for NLP provide a sort of equivalent function, e.g., \n",
    "   gensim.corpora.dictionary.Dictionary -- https://radimrehurek.com/gensim/corpora/dictionary.html#\n",
    "   tf.keras.preprocessing.text.Tokenizer -- https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "\n",
    "But we'll do it with our own function to fully understand what's going on behind the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2234680a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of tokens in dataset = 1150\n",
      "most frequent tokens:\n",
      "   good                  1127\n",
      "   great                 774\n",
      "   other                 728\n",
      "   bad                   696\n",
      "   more                  620\n",
      "   first                 545\n",
      "   little                500\n",
      "   many                  473\n",
      "   best                  408\n",
      "   much                  360\n",
      "   real                  352\n",
      "   old                   350\n",
      "   same                  345\n",
      "   better                335\n",
      "   most                  332\n",
      "   few                   307\n",
      "   funny                 304\n",
      "   only                  277\n",
      "   own                   274\n",
      "   young                 269\n",
      "\n",
      "least frequent tokens:\n",
      "   passable              5\n",
      "   out                   5\n",
      "   cosmic                5\n",
      "   annoyed               5\n",
      "   destructive           5\n",
      "   macho                 5\n",
      "   unpleasant            5\n",
      "   virtual               5\n",
      "   automatic             5\n",
      "   lifeless              5\n",
      "   bare                  5\n",
      "   pink                  5\n",
      "   cannibalistic         5\n",
      "   derivative            5\n",
      "   sappy                 5\n",
      "   horrendous            5\n",
      "   redundant             5\n",
      "   happier               5\n",
      "   hackneyed             5\n",
      "   topless               5\n"
     ]
    }
   ],
   "source": [
    "def get_token_counts(idata, use_lemma = False, reserved = (),  pos_filter=('N', 'V', 'J', 'R')):\n",
    "    '''\n",
    "    Create vocabulary from a bunch of (tokenized) texts. If use_lemma == True, use lemma rather than\n",
    "    tokens. The tuple 'reserved' can be provided to initialize the list of tokens with reserved\n",
    "    names (e.g., [PAD], [UNK], [START], [END])\n",
    "    \n",
    "    No filtering on the tokens is implemented.\n",
    "    \n",
    "    Returns:\n",
    "        - token to id mapping (dict)\n",
    "        - token count (dict)\n",
    "    '''\n",
    "\n",
    "    tokcnt = {x: 0 for x in reserved} \n",
    "    \n",
    "    kw = 'lemma' if use_lemma else 'token'\n",
    "    \n",
    "    for sample in idata:\n",
    "        \n",
    "        utterance = sample[kw]\n",
    "\n",
    "        for i, token in enumerate(utterance):\n",
    "            #\n",
    "            # if we were to implement filters, e.g., on the POS tags, that's the place\n",
    "            # where it could/should be done.\n",
    "            #\n",
    "            if sample['pos'][i][0] not in pos_filter: continue\n",
    "            \n",
    "            tokcnt[token] = 1 if token not in tokcnt else tokcnt[token] + 1\n",
    "\n",
    "    return dict(sorted(tokcnt.items(), key=lambda x: x[1], reverse = True))\n",
    "\n",
    "\n",
    "count = get_token_counts(data, pos_filter=('J'))\n",
    "\n",
    "# Set a high-frequency threshold\n",
    "high_freq_threshold = 2000  # for example, you might choose a value like the 90th percentile of frequencies\n",
    "\n",
    "# Remove high-frequency tokens from the count\n",
    "count = {token: freq for token, freq in count.items() if freq < high_freq_threshold}\n",
    "\n",
    "# Set a low-frequency threshold\n",
    "low_freq_threshold = 5  # tokens must appear at least this many times to be included\n",
    "\n",
    "# Remove low-frequency tokens from the count\n",
    "count = {token: freq for token, freq in count.items() if freq >= low_freq_threshold}\n",
    "\n",
    "#\n",
    "# Pretty print a number of things\n",
    "#\n",
    "print('total number of tokens in dataset =', len(count))\n",
    "print('most frequent tokens:')\n",
    "for x in list(count.keys())[:20]:\n",
    "    print(f\"   {x:20}  {count[x]}\")\n",
    "print('\\nleast frequent tokens:')\n",
    "for x in list(count.keys())[-20:]:\n",
    "    print(f\"   {x:20}  {count[x]}\")\n",
    "\n",
    "#\n",
    "# We select the top 2,000 tokens as our vocabulary to construct bag of words representations of the\n",
    "# documents. The vocabulary will be dictionary mapping string to ids, the latter ranging from 0 to 1,999\n",
    "#\n",
    "nterms = min(len(count), 2000)\n",
    "vocab = {x: i for i,x in enumerate(list(count.keys())[:nterms])}\n",
    "\n",
    "# ========================================================\n",
    "# TODO\n",
    "# \n",
    "# Comment the most and least frequent tokens and think about how you could get\n",
    "# a list of tokens of interest other than by selecting the most frequent ones.\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66605c",
   "metadata": {},
   "source": [
    "### Compute idf weighting\n",
    " \n",
    "Inverse document frequency (idf) is proportional to the number of samples in the database that contain a token. We want to compute that for every token in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e8646b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token = real noccs = 309 idf = 0.8110715162391466\n",
      "token = bare noccs = 9 idf = 2.3467874862246565\n"
     ]
    }
   ],
   "source": [
    "def get_num_doc(idata, token, use_lemma = False):\n",
    "    '''\n",
    "    Returns the number of samples in data that contains the specified token. If use_lemma is True,\n",
    "    search in the lemma field, else in the token field.\n",
    "    \n",
    "    Note: if a filter is implemented in get_token_counts(), this function should be adapted accordingly\n",
    "    because the wordform can become ambiguous.\n",
    "    '''\n",
    "    \n",
    "    kw = 'lemma' if use_lemma else 'token'\n",
    "    \n",
    "    return len([x for x in idata if token in x[kw]])\n",
    "\n",
    "\n",
    "#\n",
    "# Make an idf dictionary where we store the idf for each token\n",
    "#\n",
    "\n",
    "idf = np.zeros(len(vocab), dtype=np.float64)\n",
    "for term, idx in vocab.items():\n",
    "    idf[idx] = np.log10(nutterances / get_num_doc(data, term))\n",
    "\n",
    "#\n",
    "# Pretty print for control\n",
    "#\n",
    "w = list(vocab.keys())[10]\n",
    "print('token =', w, 'noccs =', get_num_doc(data, w), 'idf =', idf[10])\n",
    "\n",
    "w = list(vocab.keys())[-10]\n",
    "print('token =', w, 'noccs =', get_num_doc(data, w), 'idf =', idf[-10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ca5df",
   "metadata": {},
   "source": [
    "### Convert utterances to tf-idf vectors\n",
    "\n",
    "We first provide a function get_num_occ() that returns the number of occurrences of a given token in an utterance. Here again, this is to be adapted if you're vocabulary construction implies filtering on POS.\n",
    "\n",
    "We then use this function within a function tf_idf_vectorize() that converts a document into a tf-idf vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c895e229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', '.', 'imagine', 'a', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny', '!', 'maureen', 'stapleton', 'is', 'a', 'scene', 'stealer', '.', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream', '.', 'watch', 'for', 'alan', '\"', 'the', 'skipper', '\"', 'hale', 'jr', '.', 'as', 'a', 'police', 'sgt', '.']\n",
      "  word the       : idx=-1  noccs=2  idf=\n",
      "  word a         : idx=-1  noccs=5  idf=\n",
      "  word skipper   : idx=-1  noccs=1  idf=\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_num_occ(utterance, token, use_lemma = False):\n",
    "    '''\n",
    "    Return number of occurrences of a token in an utterance. \n",
    "    '''\n",
    "    kw = 'lemma' if use_lemma else 'token'\n",
    "    \n",
    "    filtered = list(filter(lambda x: x == token, utterance[kw]))\n",
    "    \n",
    "    return len(filtered)\n",
    "\n",
    "print(data[0]['token'])\n",
    "for w in ('the', 'a', 'skipper'):\n",
    "    idx = vocab[w] if w in vocab else -1\n",
    "    print('  word {:10}: idx={}  noccs={}  idf={}'.format(w, idx, get_num_occ(data[0], w), idf[idx] if idx != -1 else ''))\n",
    "\n",
    "# X = np.empty((nutterances, ntokens), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3593a9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', '.', 'imagine', 'a', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny', '!', 'maureen', 'stapleton', 'is', 'a', 'scene', 'stealer', '.', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream', '.', 'watch', 'for', 'alan', '\"', 'the', 'skipper', '\"', 'hale', 'jr', '.', 'as', 'a', 'police', 'sgt', '.']\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tf_idf_vectorize(utt, voc, idf, use_lemma = False):\n",
    "    '''\n",
    "    Convert utterance utt to tf-idf vector with vocabulary in voc.\n",
    "    \n",
    "    Note that this is by far not an efficient implementation but you can at least follow \n",
    "    the different steps.\n",
    "    '''\n",
    "    \n",
    "    vec = np.zeros(len(voc), dtype=np.float64) \n",
    "    \n",
    "    # get number of occurrences of each term -- remember the vocabulary is a mapping from string to index/dimension\n",
    "    for term, idx in voc.items():\n",
    "        vec[idx] = get_num_occ(utt, term, use_lemma)\n",
    "    \n",
    "    # normalize number of occurrences to get a term frequency\n",
    "    vec = vec / sum(vec)\n",
    "    \n",
    "    # multiply tf by idf\n",
    "\n",
    "    return np.multiply(vec, idf)\n",
    "\n",
    "print(data[0]['token'])\n",
    "print(tf_idf_vectorize(data[0], vocab, idf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "051b2ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/2000 [00:00<00:26, 75.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:32<00:00, 61.12it/s]\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((nutterances, nterms), dtype=np.float64)\n",
    "for i in tqdm(range(nutterances)):\n",
    "    X[i,:] = tf_idf_vectorize(data[i], vocab, idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e85a18",
   "metadata": {},
   "source": [
    "### Train k-nn index to be able to retrieve k-nearest neighbors easily and test on test data\n",
    "\n",
    "We use the training data in X to train a k-nn search with cosine distance, prepare test data and run classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "94812de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For utterance 0, the closest points are at indices: [   0 1466  514  880   72]\n",
      "For utterance 0, the closest points are at distance: [0.         0.66315446 0.67192486 0.67370548 0.69005819]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=5, metric='cosine').fit(X)\n",
    "\n",
    "#\n",
    "# What if we search for the k-nearest neighbors of each of the samples in the dataset?\n",
    "#\n",
    "dist, idx = knn.kneighbors(X)\n",
    "print('For utterance 0, the closest points are at indices:', idx[0])\n",
    "print('For utterance 0, the closest points are at distance:', dist[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d599ae41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:06<00:00, 62.83it/s]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Let's prepare test data as we did for train data\n",
    "#\n",
    "\n",
    "# Step 1. tokenize, POS tag and lemmatize test data\n",
    "database = imdb_data_test\n",
    "ntests = len(database)\n",
    "\n",
    "test_raw_texts = [x[1] for x in database]\n",
    "test_processed_texts = list(process.pipe(test_raw_texts, disable=['parser', 'ner']))\n",
    "test_data = []\n",
    "\n",
    "for i in range(ntests):\n",
    "    buf = {}\n",
    "    buf['label'] = database[i][0]\n",
    "    buf['token'] = [t.text.lower() for t in test_processed_texts[i]]\n",
    "    buf['pos'] = [t.tag_ for t in test_processed_texts[i]]\n",
    "    buf['lemma'] = [t.lemma_ for t in test_processed_texts[i]]    # already lowercased\n",
    "    \n",
    "    test_data.append(buf)\n",
    "\n",
    "# Step 2. Vectorize tes = 2000  # Yt data\n",
    "Y = np.zeros((ntests, nterms), dtype=np.float64)\n",
    "for i in tqdm(range(ntests)):\n",
    "    Y[i,:] = tf_idf_vectorize(test_data[i], vocab, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "14b1bf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test utterance 0, the closest points are at indices: [ 275 1061 1571 1219 1727]\n",
      "For test utterance 0, the closest points are at distance: [0.5840519  0.60245583 0.60843987 0.68353365 0.68587103]\n",
      "Class for test utterance 0: neg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:00<00:00, 184811.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% + utterances correct = 67.50\n",
      "% - utterances correct = 69.00\n",
      "% utterances correct = 68.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_class(neighbors, th):\n",
    "    '''\n",
    "    Return class given the neighbors, assuming all neighbor indices below th are positive ones, \n",
    "    the remaining ones being negative.\n",
    "    '''\n",
    "    \n",
    "    npos = len(list(filter(lambda i: i < th, neighbors))) # count the number of positive neighbors\n",
    "    nneg = len(neighbors) - npos\n",
    "\n",
    "    return 'pos' if npos > nneg else 'neg'\n",
    " \n",
    "    \n",
    "dist, idx = knn.kneighbors(Y)\n",
    "\n",
    "print('For test utterance 0, the closest points are at indices:', idx[0])\n",
    "print('For test utterance 0, the closest points are at distance:', dist[0])\n",
    "print('Class for test utterance 0:', get_class(idx[0], ntrain_per_class))    \n",
    "\n",
    "\n",
    "nok = [0, 0]\n",
    "for i in tqdm(range(ntests)):\n",
    "    c = get_class(idx[i], ntrain_per_class)\n",
    "    if i < ntest_per_class and c == 'pos':\n",
    "        nok[0] += 1\n",
    "    elif i >= ntest_per_class and c == 'neg':\n",
    "        nok[1] += 1\n",
    "\n",
    "print('% + utterances correct = {:.2f}'.format(100 * nok[0] / ntest_per_class))\n",
    "print('% - utterances correct = {:.2f}'.format(100 * nok[1] / ntest_per_class))\n",
    "print('% utterances correct = {:.2f}'.format(100 * (nok[0] + nok[1]) / (2 * ntest_per_class)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d5ef5",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "We've been through the whole process: \n",
    "  1. utterance processing: tokenization, POS tagging, lemmmatization\n",
    "  2. term selection: in this example, simply the most frequent tokens\n",
    "  3. utterance vectorization: tf-idf weighting of the index term\n",
    "  4. train k-nn index: train index structure for fast k-nn search\n",
    "  5. k-nn classification: classify test data with k-nn classifier\n",
    "\n",
    "Now it's your turn to improve things via better term selection. Up to you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENSAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
