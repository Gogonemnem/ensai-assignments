{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device = 'cpu' # cuda brings more headaches than answers for this exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "   def __init__(self, csv_file, root_dir, transform=None):\n",
    "       self.data_frame = pd.read_csv(csv_file)\n",
    "       self.root_dir = root_dir\n",
    "       self.transform = transform\n",
    "\n",
    "   def __len__(self):\n",
    "       return len(self.data_frame)\n",
    "\n",
    "   def __getitem__(self, idx):\n",
    "       if torch.is_tensor(idx):\n",
    "           idx = idx.tolist()\n",
    "\n",
    "       img_name = os.path.join(self.root_dir, self.data_frame.iloc[idx, 0])\n",
    "       image = Image.open(img_name)\n",
    "       label = self.data_frame.iloc[idx, 1]\n",
    "\n",
    "       if self.transform:\n",
    "           image = self.transform(image)\n",
    "\n",
    "       return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 0.9882, 0.8235, 0.4549, 0.4549, 0.4510,\n",
       "           0.4549, 0.3294, 0.0000, 0.0000, 0.0824, 0.4549, 0.4549, 0.4549,\n",
       "           0.4941, 0.9059, 0.9804, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 0.9059, 0.0039, 0.0039, 0.0039, 0.0000,\n",
       "           0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0000, 0.0039, 0.5294, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 0.9059, 0.0039, 0.0039, 0.0039, 0.0000,\n",
       "           0.0039, 0.0784, 0.0510, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
       "           0.0000, 0.0039, 0.6941, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 0.9412, 0.3686, 0.3686, 0.3686, 0.3647,\n",
       "           0.5725, 0.5373, 0.8549, 0.8235, 0.7333, 0.3686, 0.3686, 0.3686,\n",
       "           0.3333, 0.0039, 0.8235, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           0.2000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7059,\n",
       "           0.0000, 0.1176, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.5412,\n",
       "           0.0000, 0.3137, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.1686,\n",
       "           0.0314, 0.7725, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.4627, 0.0118,\n",
       "           0.0902, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.1765, 0.0000,\n",
       "           0.3373, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9255, 0.1294, 0.0039,\n",
       "           0.8745, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.5569, 0.0039, 0.0039,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2745, 0.0039, 0.2078,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 0.9804, 0.2431, 0.0000, 0.4941,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 0.7294, 0.0039, 0.0706, 0.9294,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 0.3647, 0.0039, 0.3686, 1.0000,\n",
       "           1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]]),\n",
       " 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "train_dataset = CustomDataset('data/Data1/Data1/train.csv', \"data/Data1/Data1\", transform=transform)\n",
    "test_dataset = CustomDataset('data/Data1/Data1/test.csv', \"data/Data1/Data1\", transform=transform)\n",
    "train_dataset[300-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_and_limit_dataset(dataset, num_samples=None, labels_list=None):\n",
    "    \"\"\"\n",
    "    Filters and limits the dataset to num_samples and maps the labels to 0 and 1.\n",
    "\n",
    "    dataset: The dataset to filter (e.g., MNIST).\n",
    "    num_samples: The number of samples to include in the filtered dataset.\n",
    "    labels_list: A list or set of two labels to include in the dataset.\n",
    "    \"\"\"\n",
    "    if labels_list is not None:\n",
    "        label_to_binary_map = {label: idx for idx, label in enumerate(labels_list)}\n",
    "        filtered_dataset = [(img, label_to_binary_map[label]) for img, label in dataset if label in labels_list]\n",
    "    else:\n",
    "        # Create a list from the entire dataset if no labels are specified\n",
    "        filtered_dataset = list(dataset)\n",
    "\n",
    "    if num_samples is not None:\n",
    "        return filtered_dataset[:num_samples]\n",
    "    return filtered_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and limit the datasets\n",
    "labels_07 = {0, 7}\n",
    "\n",
    "train_dataset_07 = filter_and_limit_dataset(train_dataset, labels_list = labels_07)\n",
    "test_dataset_07 = filter_and_limit_dataset(test_dataset, labels_list = labels_07)\n",
    "train_loader_07 = DataLoader(train_dataset_07, batch_size=32, shuffle=True)\n",
    "test_loader_07 = DataLoader(test_dataset_07, batch_size=32)\n",
    "img_size = train_dataset[0][0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBinaryMLPMultiple(torch.nn.Module):\n",
    "    def __init__(self, img_size=24,  n_classes=2):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.l1 = torch.nn.Linear(img_size**2, 30)\n",
    "        self.l2 = torch.nn.Linear(30, 15)\n",
    "        self.l3 = torch.nn.Linear(15, n_classes)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        flattened_image = image.view(-1, self.img_size**2)\n",
    "        x = self.l1(flattened_image)\n",
    "        x = self.l2(x)\n",
    "        x = self.softmax(self.l3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, dataloader, optimizer, loss_function):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (image, target) in enumerate(dataloader):\n",
    "        image = image.to(device)\n",
    "        target = target.to(device).long()\n",
    "        outputs = model(image)\n",
    "        loss = loss_function(outputs, target)\n",
    "\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            current = (batch + 1) * len(target)\n",
    "            print(f\"\\rEpoch: {epoch}, loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\", end=\"\")\n",
    "    print(f\"\\rEpoch: {epoch}, loss: {loss.item():>7f}  [{size:>5d}/{size:>5d}]\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No gradient needed\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subquestion 1:\n",
    "The performance is quite good for this relatively simple dataset with an accuracy close to 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, loss: 0.654177  [   32/  200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99, loss: 0.547628  [  200/  200]\n",
      "Test Accuracy: 97.83%\n"
     ]
    }
   ],
   "source": [
    "mlp_model_07 = SimpleBinaryMLPMultiple()\n",
    "mlp_model_07.to(device)\n",
    "\n",
    "mlp_loss_fn_07 = torch.nn.CrossEntropyLoss()\n",
    "mlp_optimizer_07 = torch.optim.Adam(params =  mlp_model_07.parameters(), lr=1e-05)\n",
    "\n",
    "for epoch in range(100):\n",
    "    train(mlp_model_07, epoch, train_loader_07, mlp_optimizer_07, mlp_loss_fn_07)\n",
    "print()\n",
    "    \n",
    "# Calculate accuracy on test data\n",
    "mlp_test_accuracy_07 = validate(test_loader_07, mlp_model_07)\n",
    "print(f'Test Accuracy: {mlp_test_accuracy_07:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutional feature extractor\n",
    "        self.feature_extractor = torch.nn.Sequential(            \n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=8, kernel_size=9, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "            torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        # Fully connected classifier\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(16 * 2 * 2, 15),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(15, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        logits = self.classifier(x)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subquestion 2:\n",
    "The performance is quite poor for this relatively simple dataset. It is likely that the training loss has not quite converged yet after 100 epochs. Reason could be that the fewer number of free parameters is restricting its ability too learn. Thus it needs more epochs to train. Indeed a larger number of epochs improves the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, loss: 0.693621  [   32/  200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99, loss: 0.679119  [  200/  200]\n",
      "Test Accuracy: 95.33%\n"
     ]
    }
   ],
   "source": [
    "conv_model_07 = ConvNet(2)\n",
    "conv_model_07.to(device)\n",
    "\n",
    "conv_loss_fn_07 = torch.nn.CrossEntropyLoss()\n",
    "conv_optimizer_07 = torch.optim.Adam(params =  conv_model_07.parameters(), lr=1e-05)\n",
    "\n",
    "for epoch in range(100):\n",
    "    train(conv_model_07, epoch, train_loader_07, conv_optimizer_07, conv_loss_fn_07)\n",
    "print()\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "conv_test_accuracy_07 = validate(test_loader_07, conv_model_07)\n",
    "print(f'Test Accuracy: {conv_test_accuracy_07:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and limit the datasets\n",
    "labels_017 = {0, 1, 7}\n",
    "\n",
    "train_dataset_017 = filter_and_limit_dataset(train_dataset, labels_list = labels_017)\n",
    "test_dataset_017 = filter_and_limit_dataset(test_dataset, labels_list = labels_017)\n",
    "\n",
    "train_loader_017 = DataLoader(train_dataset_017, batch_size=32, shuffle=True)\n",
    "test_loader_017 = DataLoader(test_dataset_017, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subquestion 3\n",
    "Once again the performance of the MLP is quite a bit better when we use 100 epochs for both times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99, loss: 0.978091  [  300/  300]\n",
      "Test Accuracy: 71.22%\n"
     ]
    }
   ],
   "source": [
    "mlp_model_017 = SimpleBinaryMLPMultiple(n_classes=3)\n",
    "mlp_model_017.to(device)\n",
    "\n",
    "mlp_loss_fn_017 = torch.nn.CrossEntropyLoss()\n",
    "mlp_optimizer_017 = torch.optim.Adam(params =  mlp_model_017.parameters(), lr=1e-05)\n",
    "\n",
    "for epoch in range(100):\n",
    "    train(mlp_model_017, epoch, train_loader_017, mlp_optimizer_017, mlp_loss_fn_017)\n",
    "print()\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "mlp_test_accuracy_017 = validate(test_loader_017, mlp_model_017)\n",
    "print(f'Test Accuracy: {mlp_test_accuracy_017:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99, loss: 1.076388  [  300/  300]\n",
      "Test Accuracy: 63.56%\n"
     ]
    }
   ],
   "source": [
    "conv_model_017 = ConvNet(n_classes=3)\n",
    "conv_model_017.to(device)\n",
    "\n",
    "conv_loss_fn_017 = torch.nn.CrossEntropyLoss()\n",
    "conv_optimizer_017 = torch.optim.Adam(params =  conv_model_017.parameters(), lr=1e-05)\n",
    "\n",
    "for epoch in range(100):\n",
    "    train(conv_model_017, epoch, train_loader_017, conv_optimizer_017, conv_loss_fn_017)\n",
    "print()\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "conv_test_accuracy_017 = validate(test_loader_017, conv_model_017)\n",
    "print(f'Test Accuracy: {conv_test_accuracy_017:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subquestion 4\n",
    "\n",
    "The mlp model seem to perform better on a fewer number of epochs. However, the fact that the convolutional network has less parameters than the MLP model, it is more attractive to use the conv model. Furthermore, the same number of epochs for both models still take a different amount of computation time and power as there are less parameters. Thus, for a fairer comparison, the number of epochs may be increased for the convolutional model. In that case the convolutional model will attain similar performance and will still be cheaper to train.. Furthermore, the Conv network is shift invariant whereas the FC network is not. So you need to train of way more data samples. The MLP layer has to learn the feature for each point whereas the conv model recognizes the important of, e.g., horizontal lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x_train_raw = pd.read_csv(\"data/Data2/x_train.csv\")\n",
    "x_test_raw = pd.read_csv(\"data/Data2/x_test.csv\")\n",
    "y_train_raw = pd.read_csv(\"data/Data2/y_train.csv\")\n",
    "y_test_raw = pd.read_csv(\"data/Data2/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   y       1000 non-null   int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 7.9 KB\n"
     ]
    }
   ],
   "source": [
    "y_train_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subquestion 1\n",
    "The means, and other statistics seem quite different in multiple variables for the different classes. It should be relatively easy to separate the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">V1</th>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.045940</td>\n",
       "      <td>2.989580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.057304</td>\n",
       "      <td>1.007417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.996949</td>\n",
       "      <td>-0.232610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.790081</td>\n",
       "      <td>2.361952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.021257</td>\n",
       "      <td>2.947828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.689333</td>\n",
       "      <td>3.576546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.055742</td>\n",
       "      <td>5.861592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">V2</th>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.022644</td>\n",
       "      <td>2.987018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.011928</td>\n",
       "      <td>0.950609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.008049</td>\n",
       "      <td>0.087174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.621248</td>\n",
       "      <td>2.323923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.036778</td>\n",
       "      <td>2.965997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.678563</td>\n",
       "      <td>3.648642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.810277</td>\n",
       "      <td>5.573449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">V3</th>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.029535</td>\n",
       "      <td>2.957066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.070309</td>\n",
       "      <td>1.002615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.972257</td>\n",
       "      <td>0.133701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.670832</td>\n",
       "      <td>2.282799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.018933</td>\n",
       "      <td>3.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.733914</td>\n",
       "      <td>3.630506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.153971</td>\n",
       "      <td>5.651793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">V4</th>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.002988</td>\n",
       "      <td>3.001556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.009642</td>\n",
       "      <td>1.074493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.253220</td>\n",
       "      <td>-0.060042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.706557</td>\n",
       "      <td>2.273593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.053590</td>\n",
       "      <td>2.947312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.738174</td>\n",
       "      <td>3.700901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.639574</td>\n",
       "      <td>6.624361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">V5</th>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.003380</td>\n",
       "      <td>2.999385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.099671</td>\n",
       "      <td>0.097511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.276258</td>\n",
       "      <td>2.681255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.069921</td>\n",
       "      <td>2.933129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.002996</td>\n",
       "      <td>2.998120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.063152</td>\n",
       "      <td>3.074011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.273697</td>\n",
       "      <td>3.311120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">V6</th>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.000633</td>\n",
       "      <td>3.002319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.098279</td>\n",
       "      <td>0.097303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.367130</td>\n",
       "      <td>2.726102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.063938</td>\n",
       "      <td>2.935750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.002351</td>\n",
       "      <td>3.004296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.061635</td>\n",
       "      <td>3.063387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.256509</td>\n",
       "      <td>3.295814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">V7</th>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.005641</td>\n",
       "      <td>3.001957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.100860</td>\n",
       "      <td>0.095616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.320806</td>\n",
       "      <td>2.677277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.068926</td>\n",
       "      <td>2.939954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.009222</td>\n",
       "      <td>3.004342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.065634</td>\n",
       "      <td>3.063280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.263413</td>\n",
       "      <td>3.309340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">V8</th>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.008986</td>\n",
       "      <td>2.993774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.106385</td>\n",
       "      <td>0.103622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.288502</td>\n",
       "      <td>2.654950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.067012</td>\n",
       "      <td>2.929445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.008584</td>\n",
       "      <td>2.988703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.083562</td>\n",
       "      <td>3.063111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.306452</td>\n",
       "      <td>3.337691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">V9</th>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.002152</td>\n",
       "      <td>2.999850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.102582</td>\n",
       "      <td>0.100867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.278601</td>\n",
       "      <td>2.699597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.059081</td>\n",
       "      <td>2.927772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001448</td>\n",
       "      <td>2.997861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.068950</td>\n",
       "      <td>3.065862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.286214</td>\n",
       "      <td>3.295537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">V10</th>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000910</td>\n",
       "      <td>2.999308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.103736</td>\n",
       "      <td>0.099435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.353959</td>\n",
       "      <td>2.685295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.066892</td>\n",
       "      <td>2.937845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.004823</td>\n",
       "      <td>3.004080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.076928</td>\n",
       "      <td>3.069519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.268392</td>\n",
       "      <td>3.232944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "y                   0           1\n",
       "V1  count  500.000000  500.000000\n",
       "    mean    -0.045940    2.989580\n",
       "    std      1.057304    1.007417\n",
       "    min     -2.996949   -0.232610\n",
       "    25%     -0.790081    2.361952\n",
       "    50%     -0.021257    2.947828\n",
       "    75%      0.689333    3.576546\n",
       "    max      3.055742    5.861592\n",
       "V2  count  500.000000  500.000000\n",
       "    mean     0.022644    2.987018\n",
       "    std      1.011928    0.950609\n",
       "    min     -3.008049    0.087174\n",
       "    25%     -0.621248    2.323923\n",
       "    50%     -0.036778    2.965997\n",
       "    75%      0.678563    3.648642\n",
       "    max      3.810277    5.573449\n",
       "V3  count  500.000000  500.000000\n",
       "    mean    -0.029535    2.957066\n",
       "    std      1.070309    1.002615\n",
       "    min     -2.972257    0.133701\n",
       "    25%     -0.670832    2.282799\n",
       "    50%     -0.018933    3.028300\n",
       "    75%      0.733914    3.630506\n",
       "    max      3.153971    5.651793\n",
       "V4  count  500.000000  500.000000\n",
       "    mean    -0.002988    3.001556\n",
       "    std      1.009642    1.074493\n",
       "    min     -3.253220   -0.060042\n",
       "    25%     -0.706557    2.273593\n",
       "    50%     -0.053590    2.947312\n",
       "    75%      0.738174    3.700901\n",
       "    max      3.639574    6.624361\n",
       "V5  count  500.000000  500.000000\n",
       "    mean    -0.003380    2.999385\n",
       "    std      0.099671    0.097511\n",
       "    min     -0.276258    2.681255\n",
       "    25%     -0.069921    2.933129\n",
       "    50%     -0.002996    2.998120\n",
       "    75%      0.063152    3.074011\n",
       "    max      0.273697    3.311120\n",
       "V6  count  500.000000  500.000000\n",
       "    mean    -0.000633    3.002319\n",
       "    std      0.098279    0.097303\n",
       "    min     -0.367130    2.726102\n",
       "    25%     -0.063938    2.935750\n",
       "    50%      0.002351    3.004296\n",
       "    75%      0.061635    3.063387\n",
       "    max      0.256509    3.295814\n",
       "V7  count  500.000000  500.000000\n",
       "    mean    -0.005641    3.001957\n",
       "    std      0.100860    0.095616\n",
       "    min     -0.320806    2.677277\n",
       "    25%     -0.068926    2.939954\n",
       "    50%     -0.009222    3.004342\n",
       "    75%      0.065634    3.063280\n",
       "    max      0.263413    3.309340\n",
       "V8  count  500.000000  500.000000\n",
       "    mean     0.008986    2.993774\n",
       "    std      0.106385    0.103622\n",
       "    min     -0.288502    2.654950\n",
       "    25%     -0.067012    2.929445\n",
       "    50%      0.008584    2.988703\n",
       "    75%      0.083562    3.063111\n",
       "    max      0.306452    3.337691\n",
       "V9  count  500.000000  500.000000\n",
       "    mean     0.002152    2.999850\n",
       "    std      0.102582    0.100867\n",
       "    min     -0.278601    2.699597\n",
       "    25%     -0.059081    2.927772\n",
       "    50%      0.001448    2.997861\n",
       "    75%      0.068950    3.065862\n",
       "    max      0.286214    3.295537\n",
       "V10 count  500.000000  500.000000\n",
       "    mean     0.000910    2.999308\n",
       "    std      0.103736    0.099435\n",
       "    min     -0.353959    2.685295\n",
       "    25%     -0.066892    2.937845\n",
       "    50%     -0.004823    3.004080\n",
       "    75%      0.076928    3.069519\n",
       "    max      0.268392    3.232944"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# Combine x_train and y_train into a single DataFrame\n",
    "combined = x_train_raw.copy()\n",
    "combined['y'] = y_train_raw['y']\n",
    "\n",
    "# Group by the class and describe\n",
    "combined.groupby('y').describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_raw)\n",
    "x_test_scaled = scaler.transform(x_test_raw)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "x_test = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_raw.values, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test_raw.values, dtype=torch.long)\n",
    "\n",
    "# Determine the input size and number of classes\n",
    "input_size = x_train.shape[1]\n",
    "num_classes = len(y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 30) \n",
    "        self.fc2 = nn.Linear(30, 15)\n",
    "        self.fc3 = nn.Linear(15, num_classes)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(model, criterion, optimizer, x_train, y_train, epochs=100):\n",
    "    model.train()\n",
    "\n",
    "    # Ensure y_train is a 1D tensor if it's not already\n",
    "    if y_train.ndim > 1:\n",
    "        y_train = y_train.squeeze()\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'\\rEpoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}', end=\"\")\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate2(model, x_test, y_test):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    if y_test.ndim > 1:\n",
    "        y_test = y_test.squeeze()\n",
    "    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        outputs = model(x_test)\n",
    "        predicted = torch.argmax(outputs.data, 1)  # Get the predicted classes\n",
    "        total += y_test.size(0)\n",
    "        correct += (predicted == y_test).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.4f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subquestion 2\n",
    "\n",
    "The model is performing quite poor (and very random, initialization matters a lot), (sometimes) only around 50 percent correct meaning that the model is about as good as a random guess. With the descriptive statistics, this result is quite shocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.6978\n",
      "Accuracy: 41.9000%\n"
     ]
    }
   ],
   "source": [
    "# Create the MLP model\n",
    "model = MLP(input_size, num_classes=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=1e-05)\n",
    "\n",
    "train2(model, loss_fn, optimizer, x_train, y_train, epochs=100)\n",
    "\n",
    "# Call the evaluate function\n",
    "evaluate2(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(DenseAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 10),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10, 6),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(6, 4),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, 6),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(6, 10),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10, input_size)  # Output size is same as input size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subquestion 3\n",
    "The model seems to learn quite well how the data is constructed. It does quite well in autoencoding the (test) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate3(model, criterion, x_test, y_test):\n",
    "    model.eval()    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(x_test)\n",
    "        loss = criterion(outputs, y_test)\n",
    "        print(f'Eval Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.2475\n",
      "Eval Loss: 0.2392\n"
     ]
    }
   ],
   "source": [
    "# Assuming input size of 10 (to match your layer sizes)\n",
    "input_size = 10\n",
    "model = DenseAutoencoder(input_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train2(model, criterion, optimizer, x_train, x_train)\n",
    "evaluate3(model, criterion, x_test, x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodedMLP(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(EncodedMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 8)  # First layer\n",
    "        self.fc2 = nn.Linear(8, 4)  # Second layer\n",
    "        self.fc3 = nn.Linear(4, num_classes)  # final\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(model, x):\n",
    "    # Extract the first three layers from the encoder part of the model\n",
    "    with torch.no_grad():\n",
    "        x = model.encoder(x)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subquestion 4\n",
    "This model that takes the encoded form of the data seems to be performing better than the original NN. We can draw parallels with transfer learning that this data profits from understanding the data in a different way, without training the previously learned transformations. However, I must confess that I do not have a definite answer as why the encoded model performs much better than the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.5861\n",
      "Accuracy: 100.0000%\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "encoded_model = EncodedMLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(encoded_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Transform training and testing data\n",
    "x_train_transformed = encode_data(model, x_train)\n",
    "x_test_transformed = encode_data(model, x_test)\n",
    "\n",
    "# Train the third model\n",
    "train2(encoded_model, criterion, optimizer, x_train_transformed, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate2(encoded_model, x_test_transformed, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENSAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
